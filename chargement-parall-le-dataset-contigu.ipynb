{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Google colab\n\nPour utiliser le google drive et avoir des informations sur le système utilisé.","metadata":{"id":"20945XbxGAph"}},{"cell_type":"code","source":"#from google.colab import drive\n#drive.mount('/content/drive')\n\n#chemin_google_drive = \"drive/MyDrive/\"\n\n!cat /proc/cpuinfo | grep 'model name'\n!cat /proc/meminfo | grep 'MemTotal'","metadata":{"id":"xTtAqObTBBgm","outputId":"2f8ff909-9796-4dbb-f0db-c744dc140ddf","execution":{"iopub.status.busy":"2023-08-04T12:15:58.657465Z","iopub.execute_input":"2023-08-04T12:15:58.657843Z","iopub.status.idle":"2023-08-04T12:16:00.692257Z","shell.execute_reply.started":"2023-08-04T12:15:58.657806Z","shell.execute_reply":"2023-08-04T12:16:00.690865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Sinon \n\nchemin_train = \"/kaggle/input/abc-dataset-train/\"\nchemin_validation = \"/kaggle/input/abc-dataset-validation/\"","metadata":{"execution":{"iopub.status.busy":"2023-08-04T12:16:00.695227Z","iopub.execute_input":"2023-08-04T12:16:00.695573Z","iopub.status.idle":"2023-08-04T12:16:00.701754Z","shell.execute_reply.started":"2023-08-04T12:16:00.695544Z","shell.execute_reply":"2023-08-04T12:16:00.700795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Chargement fichier .ply\n\n\nPour charger un fichier.ply, on utilise la librarie plyfile (https://pypi.org/project/plyfile/). Remarque on utilisera seulement plyfile pour écrire des fichiers .ply puisqu'il se trouve qu'il n'est pas adapté pour lire nos fichiers ply (on lit que les 3 premières colonnes),ce qui donne un temps de lecture très lent par fichier. On utilisera aussi la librarie tqdm pour avoir une barre de progression (et donc le temps).","metadata":{"id":"f4cf0156"}},{"cell_type":"code","source":"!pip install plyfile","metadata":{"id":"RKMAF_7RA5RJ","outputId":"54310d61-ba1a-4ab2-f614-b55af25cd2b2","execution":{"iopub.status.busy":"2023-08-04T12:16:00.703444Z","iopub.execute_input":"2023-08-04T12:16:00.704218Z","iopub.status.idle":"2023-08-04T12:16:14.218467Z","shell.execute_reply.started":"2023-08-04T12:16:00.704171Z","shell.execute_reply":"2023-08-04T12:16:14.217221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from plyfile import PlyData, PlyElement\nfrom tqdm import tqdm","metadata":{"id":"11b67675","execution":{"iopub.status.busy":"2023-08-04T12:16:14.221958Z","iopub.execute_input":"2023-08-04T12:16:14.222314Z","iopub.status.idle":"2023-08-04T12:16:14.233938Z","shell.execute_reply.started":"2023-08-04T12:16:14.222283Z","shell.execute_reply":"2023-08-04T12:16:14.232989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(chemin_train+'0002.ply', 'rb') as f:\n    plydata = PlyData.read(f)","metadata":{"id":"fdd5ade5","execution":{"iopub.status.busy":"2023-08-04T12:16:14.235608Z","iopub.execute_input":"2023-08-04T12:16:14.235974Z","iopub.status.idle":"2023-08-04T12:16:14.953434Z","shell.execute_reply.started":"2023-08-04T12:16:14.235943Z","shell.execute_reply":"2023-08-04T12:16:14.952286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualisation fichier .ply\n\nPour confirmer le chargement, on utilise la fonction de scatter de matploblib pour visualiser le nuage de points (https://matplotlib.org/stable/gallery/mplot3d/scatter3d.html).","metadata":{"id":"6c79c476"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"id":"869123c3","execution":{"iopub.status.busy":"2023-08-04T12:16:14.954843Z","iopub.execute_input":"2023-08-04T12:16:14.955355Z","iopub.status.idle":"2023-08-04T12:16:14.962227Z","shell.execute_reply.started":"2023-08-04T12:16:14.95532Z","shell.execute_reply":"2023-08-04T12:16:14.960081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def afficher(plydata):\n    x = plydata.elements[0].data['x']\n    y = plydata.elements[0].data['y']\n    z = plydata.elements[0].data['z']\n    red = plydata.elements[0].data['red']\n    green = plydata.elements[0].data['green']\n    blue = plydata.elements[0].data['blue']\n    fig = plt.figure(figsize=(6, 6))\n    ax = fig.add_subplot(projection='3d')\n\n    colors = [(r/255,g/255,b/255,1) for r,g,b in zip(red, green, blue)]\n    ax.scatter(x, y, z, marker='.', s=1, c=colors)\n\n    ax.set_xlabel('X Label')\n    ax.set_ylabel('Y Label')\n    ax.set_zlabel('Z Label')\n\n    plt.show()","metadata":{"id":"34b87c04","execution":{"iopub.status.busy":"2023-08-04T12:16:14.963744Z","iopub.execute_input":"2023-08-04T12:16:14.964349Z","iopub.status.idle":"2023-08-04T12:16:14.975081Z","shell.execute_reply.started":"2023-08-04T12:16:14.964314Z","shell.execute_reply":"2023-08-04T12:16:14.974004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"afficher(plydata)","metadata":{"id":"75966dec","outputId":"e7929c68-e41f-4ad4-d927-2541774c0c0a","execution":{"iopub.status.busy":"2023-08-04T12:16:14.976548Z","iopub.execute_input":"2023-08-04T12:16:14.977126Z","iopub.status.idle":"2023-08-04T12:16:16.041437Z","shell.execute_reply.started":"2023-08-04T12:16:14.977087Z","shell.execute_reply":"2023-08-04T12:16:16.040443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Traitement des données .ply\n\nPour utiliser le .ply dans notre modèle PyTorch (https://pytorch.org/), on passera par un Dataset modifié qui lors du chargement du .ply rajoutera les kppv comme information à l'aide de la librarie sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).","metadata":{"id":"dea3105d"}},{"cell_type":"code","source":"import torch\nfrom sklearn.neighbors import NearestNeighbors\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np","metadata":{"id":"1ed1e829","execution":{"iopub.status.busy":"2023-08-04T12:16:16.042504Z","iopub.execute_input":"2023-08-04T12:16:16.042871Z","iopub.status.idle":"2023-08-04T12:16:20.207036Z","shell.execute_reply.started":"2023-08-04T12:16:16.042839Z","shell.execute_reply":"2023-08-04T12:16:20.205863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, ply_data, lb_data, k):\n        self.k = k+1\n        self.hasLabel = True\n        if lb_data == None:\n            self.hasLabel = False\n        self.data, self.labels = self.load_data(ply_data, lb_data)\n\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        label = self.labels[idx]\n\n        return sample, label\n\n    def load_data(self, ply_data, lb_data):\n        while \"end_header\" not in ply_data.readline():\n            pass     \n        \n        points = np.loadtxt(ply_data, dtype=np.float32, ndmin=2, usecols=(0, 1, 2))\n        \n        if self.hasLabel:\n            lb_data.readline()        \n            labels = np.loadtxt(lb_data, dtype=np.int64) \n        else:\n            labels = np.empty(len(points)) * np.nan\n\n\n        neighbors = NearestNeighbors(n_neighbors=self.k, n_jobs=-1)\n        neighbors.fit(points)\n        \n        distances, indices = neighbors.kneighbors(points, return_distance=True)\n        data = np.zeros((len(points), self.k*3), dtype=np.float32)\n        data[:, :3] = points        \n        data[:, 3:] = points[indices[:, 1:]].reshape(len(points), -1)\n\n        data = torch.Tensor(data)\n\n        return data, labels","metadata":{"id":"b91f489d","execution":{"iopub.status.busy":"2023-08-04T12:16:20.211144Z","iopub.execute_input":"2023-08-04T12:16:20.212037Z","iopub.status.idle":"2023-08-04T12:16:20.224027Z","shell.execute_reply.started":"2023-08-04T12:16:20.212008Z","shell.execute_reply":"2023-08-04T12:16:20.222874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Définition du modèle\n\nPour rappel notre modèle prend en entrée un point et ses kppv et donne en sortie son type (si c'est un bord ou non).","metadata":{"id":"671bb227"}},{"cell_type":"code","source":"import torch.nn as nn","metadata":{"id":"08e9b654","execution":{"iopub.status.busy":"2023-08-04T12:16:20.225512Z","iopub.execute_input":"2023-08-04T12:16:20.22647Z","iopub.status.idle":"2023-08-04T12:16:20.23509Z","shell.execute_reply.started":"2023-08-04T12:16:20.226435Z","shell.execute_reply":"2023-08-04T12:16:20.234143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, k):\n        super().__init__()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(k*3+3, k),\n            nn.ReLU(),\n            nn.Linear(k, 2)\n        )\n\n    def forward(self, x):\n        x = self.linear_relu_stack(x)\n        return x","metadata":{"id":"16f9aba7","execution":{"iopub.status.busy":"2023-08-04T12:16:20.236945Z","iopub.execute_input":"2023-08-04T12:16:20.237379Z","iopub.status.idle":"2023-08-04T12:16:20.250241Z","shell.execute_reply.started":"2023-08-04T12:16:20.237279Z","shell.execute_reply":"2023-08-04T12:16:20.249089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Données du modèle\n\nPour l'entraînement on utilisera une pièce du dossier train et pour la validation on utilisera toutes les pièces du dossier test. Et on utilisera une pièce autre que celle utilisée dans le train pour donner un exemple de résultat (pour l'affichage).","metadata":{"id":"3ba4d03c"}},{"cell_type":"code","source":"from torch.utils.data import ConcatDataset\nimport torch.multiprocessing as mp\nimport time","metadata":{"id":"995dcc8d","execution":{"iopub.status.busy":"2023-08-04T12:16:20.251862Z","iopub.execute_input":"2023-08-04T12:16:20.252312Z","iopub.status.idle":"2023-08-04T12:16:20.260902Z","shell.execute_reply.started":"2023-08-04T12:16:20.252205Z","shell.execute_reply":"2023-08-04T12:16:20.259829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def charger_datasets(chemin, fichiers):\n    datasets_list = []\n    for fichier in tqdm(fichiers):\n        with open(chemin+fichier+\".lb\", 'rb') as data_lb, \\\n            open(chemin+fichier+\".ply\", 'r') as data_ply:\n            dataset = CustomDataset(data_ply, data_lb, k)\n        datasets_list.append((dataset.data, dataset.labels))\n    return datasets_list","metadata":{"id":"XZxbr7iAaUpv","execution":{"iopub.status.busy":"2023-08-04T12:16:20.264248Z","iopub.execute_input":"2023-08-04T12:16:20.264555Z","iopub.status.idle":"2023-08-04T12:16:20.272583Z","shell.execute_reply.started":"2023-08-04T12:16:20.26453Z","shell.execute_reply":"2023-08-04T12:16:20.271463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inutile d'activer le shuffle sur le validation_dataloader et afficher_dataloader.","metadata":{"id":"S314r75oemOq"}},{"cell_type":"code","source":"class SimpleDataset(Dataset):\n    def __init__(self, data, labels):\n        self.data = data\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        labels = self.labels[idx]\n        return sample,labels","metadata":{"execution":{"iopub.status.busy":"2023-08-04T12:16:20.273968Z","iopub.execute_input":"2023-08-04T12:16:20.274436Z","iopub.status.idle":"2023-08-04T12:16:20.282737Z","shell.execute_reply.started":"2023-08-04T12:16:20.274403Z","shell.execute_reply":"2023-08-04T12:16:20.28181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def diviser_liste(liste, nombre):\n    taille_sous_liste = max(1,len(liste)//nombre)\n    return [liste[i:i + taille_sous_liste] for i in range(0, len(liste), taille_sous_liste)]\n\n\ndef wrapper_charger_datasets(chemin,fichiers, queue):\n    queue.put(charger_datasets(chemin,fichiers))\n\ndef parallele_charger_datasets(chemin, fichiers, num_cpus):    \n    resultat_queue = mp.Manager().Queue()\n    partage = diviser_liste(fichiers, num_cpus)\n\n    processes = []\n    for sous_liste in partage:\n        p = mp.Process(target=wrapper_charger_datasets, args=(chemin, sous_liste, resultat_queue))\n        processes.append(p)\n        p.start()\n\n    for p in processes:\n        p.join()\n\n    list_data = []\n    list_labels = []\n    while not resultat_queue.empty():\n        res = resultat_queue.get()\n        for (a,b) in res:\n            list_data.append(a)    \n            list_labels.append(torch.tensor(b))    \n    data = torch.cat(list_data, dim=0)\n    labels = torch.cat(list_labels, dim=0)\n    concatenated_dataset = SimpleDataset(data,labels)\n    \n    return concatenated_dataset","metadata":{"execution":{"iopub.status.busy":"2023-08-04T12:16:20.284564Z","iopub.execute_input":"2023-08-04T12:16:20.285493Z","iopub.status.idle":"2023-08-04T12:16:20.29668Z","shell.execute_reply.started":"2023-08-04T12:16:20.285456Z","shell.execute_reply":"2023-08-04T12:16:20.296022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k = 10\nbatch_size = 100\n\nafficher_data_ply = PlyData.read(chemin_train+\"0008.ply\") #0002 0008\nafficher(afficher_data_ply)\n\nwith open(chemin_train+\"0008.lb\", 'rb') as afficher_data_lb, \\\n    open(chemin_train+\"0008.ply\", 'r') as afficher_data_ply:\n    afficher_dataset = CustomDataset(afficher_data_ply, afficher_data_lb, k)\n\nafficher_dataloader = DataLoader(afficher_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n\nfichiers = ['0000', '0001', '0002', '0004', '0005', '0008', '0011', '0013', '0041', '0042', '0043', '0044', '0045', '0046', '0047', '0048', '0049', '0050', '0051', '0052', '0053', '0054', '0055', '0056', '0089', '0090', '0091', '0092', '0093', '0094', '0095', '0096', '0097', '0098', '0099', '0100', '0101', '0102', '0103', '0104', '0105', '0106', '0107', '0108', '0109', '0110', '0111', '0112', '0113', '0114', '0115', '0116', '0117', '0118', '0119', '0120', '0121', '0122', '0123', '0124', '0125', '0126', '0127', '0128', '0160', '0161', '0162', '0163', '0164', '0165', '0166', '0167', '0168', '0169', '0170', '0171', '0172', '0173', '0174', '0175', '0176', '0177', '0178', '0179', '0180', '0181', '0274', '0275', '0276', '0277', '0278', '0279', '0280', '0281', '0989', '0990', '0991', '0992', '0993', '0994', '0995', '0996', '0997', '0998']\nn=mp.cpu_count()\ntrain_dataloader = DataLoader(parallele_charger_datasets(chemin_train, fichiers, n), batch_size=batch_size, shuffle=True)  \nprint(\"Fin chargement train\")\nfichiers = ['0141', '0228', '0508', '0658', '0755', '0782', '0820', '0917', '0142', '0229', '0509', '0659', '0756', '0783', '0821', '0918', '0143', '0230', '0510', '0660', '0757', '0784', '0822', '0919', '0144', '0231', '0511', '0661', '0758', '0785', '0823', '0920', '0145', '0232', '0512', '0662', '0759', '0786', '0824', '0921', '0146', '0233', '0513', '0663', '0760', '0787', '0825', '0922', '0147', '0234', '0514', '0664', '0761', '0788', '0826', '0923', '0148', '0235', '0515', '0665', '0762', '0789', '0827', '0924', '0149', '0236', '0516', '0666', '0763', '0790', '0828', '0925', '0150', '0237', '0654', '0667', '0764', '0791', '0829', '0225', '0238', '0655', '0668', '0765', '0817', '0830', '0226', '0239', '0656', '0753', '0766', '0818', '0831', '0227', '0507', '0657', '0754', '0767', '0819', '0916']\nn=mp.cpu_count()\nvalidation_dataloader = DataLoader(parallele_charger_datasets(chemin_validation, fichiers, n), batch_size=batch_size, shuffle=False)\nprint(\"Fin chargement validation\")\n","metadata":{"id":"f89757f6","outputId":"fdf9d225-0d27-4cc4-d2e3-a145ce339973","execution":{"iopub.status.busy":"2023-08-04T12:16:20.298281Z","iopub.execute_input":"2023-08-04T12:16:20.299055Z","iopub.status.idle":"2023-08-04T12:17:34.838612Z","shell.execute_reply.started":"2023-08-04T12:16:20.299021Z","shell.execute_reply":"2023-08-04T12:17:34.837436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Accélération matérielle\n\nPour accélérer le temps d'apprentissage on utilisera un GPU (si c'est le cas).","metadata":{"id":"D608fMlKFc9J"}},{"cell_type":"code","source":"device = (\"cuda\" if torch.cuda.is_available()\n  else \"mps\" if torch.backends.mps.is_available()\n  else \"cpu\")\nprint(\"Device:\", device)\nif torch.cuda.is_available():\n    print(torch.cuda.get_device_name(0))","metadata":{"id":"BLAKv88AFj5z","outputId":"34f5bdca-390a-4075-a96f-8709efdec7f5","execution":{"iopub.status.busy":"2023-08-04T12:17:38.801129Z","iopub.execute_input":"2023-08-04T12:17:38.801518Z","iopub.status.idle":"2023-08-04T12:17:38.89007Z","shell.execute_reply.started":"2023-08-04T12:17:38.801485Z","shell.execute_reply":"2023-08-04T12:17:38.888857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Apprentissage du modèle\n\nOn définit une fonction train (d'entraînement) avec arrêt prématuré et stockage des loss du dataset train et du dataset validation.","metadata":{"id":"c7f8248d"}},{"cell_type":"code","source":"import torch.optim as optim\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt","metadata":{"id":"b2de987e","execution":{"iopub.status.busy":"2023-08-04T12:17:40.673641Z","iopub.execute_input":"2023-08-04T12:17:40.674725Z","iopub.status.idle":"2023-08-04T12:17:40.680235Z","shell.execute_reply.started":"2023-08-04T12:17:40.674679Z","shell.execute_reply":"2023-08-04T12:17:40.679126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, num_epochs, train_dataloader, val_dataloader, criterion, optimizer, train_losses, val_losses, patience):\n    count = 0\n    best_val_loss = float('inf')\n\n    for epoch in tqdm(range(num_epochs)):\n        epoch_train_loss = 0\n        epoch_val_loss = 0\n\n        model.train()\n        for i, (inputs, labels) in enumerate(train_dataloader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            epoch_train_loss += loss.item()\n\n\n        avg_epoch_train_loss = epoch_train_loss / len(train_dataloader)\n        train_losses.append(avg_epoch_train_loss)      \n\n        model.eval()\n        with torch.no_grad():\n            for inputs, labels in val_dataloader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                epoch_val_loss += loss.item()\n\n            avg_epoch_val_loss = epoch_val_loss / len(val_dataloader)\n            val_losses.append(avg_epoch_val_loss)\n\n\n            if avg_epoch_val_loss < best_val_loss:\n                best_val_loss = avg_epoch_val_loss\n                best_model_weights = model.state_dict()\n                count = 0\n            else:\n                count += 1\n                if count >= patience:\n                    print(\"Early stop\")\n                    model.load_state_dict(best_model_weights)\n                    break","metadata":{"id":"f6cbcec0","execution":{"iopub.status.busy":"2023-08-04T12:17:41.581191Z","iopub.execute_input":"2023-08-04T12:17:41.581745Z","iopub.status.idle":"2023-08-04T12:17:41.596163Z","shell.execute_reply.started":"2023-08-04T12:17:41.581706Z","shell.execute_reply":"2023-08-04T12:17:41.595272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def afficher_loss(train_losses, val_losses):\n    plt.plot(range(1,len(train_losses)+1), train_losses, marker='o', label='Train')\n    plt.plot(range(1,len(val_losses)+1), val_losses, marker='o', label='Validation')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Loss per Epoch')\n    plt.legend()\n    plt.grid(True)\n    plt.show()","metadata":{"id":"63d953c1","execution":{"iopub.status.busy":"2023-08-04T12:17:43.3213Z","iopub.execute_input":"2023-08-04T12:17:43.321678Z","iopub.status.idle":"2023-08-04T12:17:43.329086Z","shell.execute_reply.started":"2023-08-04T12:17:43.321638Z","shell.execute_reply":"2023-08-04T12:17:43.328049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pour l'apprentissage, on utilise comme loss la Crossentropy et comme optimizer Adam. On utilisera aussi un arrêt prématuré de 5 epochs sur 100 epochs maximums. Et les hypers-paramètres suivant :","metadata":{"id":"b228254b"}},{"cell_type":"code","source":"model = Model(k).to(device)\n\nlearning_rate = 0.001\nnum_epochs = 100\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\npatience = 5\n\ntrain_losses = []\nval_losses = []\n\ntrain(model, num_epochs, train_dataloader, validation_dataloader, criterion, optimizer, train_losses, val_losses, patience)\n\n# Sauvegarde du modèle entraîné\n# torch.save(model.state_dict(), \"modele.pth\")\n\nafficher_loss(train_losses, val_losses)","metadata":{"id":"db9f65d9","outputId":"8027bd70-f4ba-43d7-d77f-62070f539a03","execution":{"iopub.status.busy":"2023-08-04T11:33:07.031533Z","iopub.execute_input":"2023-08-04T11:33:07.031922Z","iopub.status.idle":"2023-08-04T11:35:25.664668Z","shell.execute_reply.started":"2023-08-04T11:33:07.031874Z","shell.execute_reply":"2023-08-04T11:35:25.662579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Résultats\n\nOn se réfère aux formules définies à la fin de l'article de PCEDNet.","metadata":{"id":"e7584c49"}},{"cell_type":"code","source":"def precision(TP,FP):\n    if (TP + FP) != 0:\n        return TP / (TP + FP)\n    else:\n        return 0\n\ndef rappel(TP,FN):\n    if (TP + FN) != 0:\n        return TP / (TP + FN)\n    else:\n        return 0\n\ndef f1_score(TP, FP, FN):\n    precision_value = precision(TP, FP)\n    recall_value = rappel(TP, FN)\n\n    if (precision_value + recall_value) != 0:\n        return 2 * (precision_value * recall_value) / (precision_value + recall_value)\n    else:\n        return 0\n\n\n\ndef mcc(TP, TN, FP, FN):\n    numerator = (TP * TN) - (FP * FN)\n    denominator = ((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)) ** 0.5\n    if denominator != 0:\n        return numerator / denominator\n    else:\n        return 0\n\n\ndef accuracy(TP, TN, FP, FN):\n    total = TP + TN + FP + FN\n    if total != 0:\n        return (TP + TN) / total\n    else:\n        return 0\n\n\ndef iou(TP, FP, FN):\n    union = TP + FP + FN\n    if union != 0:\n        return TP / union\n    else:\n        return 0","metadata":{"id":"301dc417","execution":{"iopub.status.busy":"2023-08-04T12:17:48.333756Z","iopub.execute_input":"2023-08-04T12:17:48.335084Z","iopub.status.idle":"2023-08-04T12:17:48.362972Z","shell.execute_reply.started":"2023-08-04T12:17:48.335041Z","shell.execute_reply":"2023-08-04T12:17:48.361086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resultat(model, validation_dataloader):\n    fp = 0\n    fn = 0\n    tp = 0\n    tn = 0\n\n    list_predicted = []\n    list_labels = []        \n    \n    model.eval()\n    with torch.no_grad():\n        for inputs, labels in tqdm(validation_dataloader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            list_predicted.append(predicted)\n            list_labels.append(labels)\n    \n    predicted = torch.cat(list_predicted, dim=0)\n    labels = torch.cat(list_labels, dim=0)\n\n    tp += torch.sum(torch.eq(predicted, labels) & torch.eq(labels, 1)).item()\n    tn += torch.sum(torch.eq(predicted, labels) & torch.eq(labels, 0)).item()\n\n    fp += torch.sum(torch.ne(predicted, labels) & torch.eq(labels, 0)).item()\n    fn += torch.sum(torch.ne(predicted, labels) & torch.eq(labels, 1)).item()   \n\n    print(fp, tp, fn, tn)\n\n    print(\"Précision : \", precision(tp, fp))\n    print(\"Rappel : \", rappel(tp, fn))\n    print(\"F1 score :\", f1_score(tp,fp,fn))\n    print(\"MCC : \", mcc(tp, tn, fp, fn))\n    print(\"Accuracy : \", accuracy(tp, tn, fp, fn))\n    print(\"IoU : \", iou(tp, fp, fn))","metadata":{"id":"61cc9025","execution":{"iopub.status.busy":"2023-08-04T12:18:21.519543Z","iopub.execute_input":"2023-08-04T12:18:21.519938Z","iopub.status.idle":"2023-08-04T12:18:21.533904Z","shell.execute_reply.started":"2023-08-04T12:18:21.519904Z","shell.execute_reply":"2023-08-04T12:18:21.532675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(validation_dataloader.dataset.data))","metadata":{"execution":{"iopub.status.busy":"2023-08-04T12:20:02.273696Z","iopub.execute_input":"2023-08-04T12:20:02.274724Z","iopub.status.idle":"2023-08-04T12:20:02.280447Z","shell.execute_reply.started":"2023-08-04T12:20:02.274687Z","shell.execute_reply":"2023-08-04T12:20:02.279486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultat(model, validation_dataloader)","metadata":{"id":"7001f8c5","outputId":"8a936219-bd5b-41aa-b3f1-9bff19e22bb2","execution":{"iopub.status.busy":"2023-08-04T12:18:23.500043Z","iopub.execute_input":"2023-08-04T12:18:23.500435Z","iopub.status.idle":"2023-08-04T12:18:37.13732Z","shell.execute_reply.started":"2023-08-04T12:18:23.500405Z","shell.execute_reply":"2023-08-04T12:18:37.135867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"7285/43947 [00:07<00:31, 1146.21it/s]\n\n3069/43947 [00:08<01:56, 350.45it/s] avec num_workers=1\n\n392/1099 [00:16<00:30, 23.10it/s]","metadata":{}},{"cell_type":"markdown","source":"## Affichage","metadata":{"id":"c58b164c"}},{"cell_type":"code","source":"def afficher_exemple(model, afficher_dataloader):\n    model.eval()\n    points = []\n\n    with torch.no_grad():\n        for inputs, labels in afficher_dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            for i,j in zip(inputs,predicted):\n                if j.tolist() == 0:\n                    points.append((i[0].tolist(), i[1].tolist(), i[2].tolist(), 255, 255, 255, 255))\n                else:\n                    points.append((i[0].tolist(), i[1].tolist(), i[2].tolist(), 255, 0, 0, 255))\n\n\n    properties = [('x', 'f4'), ('y', 'f4'), ('z', 'f4'), ('red', 'u1'), ('green', 'u1'), ('blue', 'u1'), ('alpha', 'u1')]\n\n    points = np.array(points, dtype=properties)\n    vertex_element = PlyElement.describe(points, 'vertex')\n\n    new_ply_data = PlyData([vertex_element])\n\n    #new_ply_data.write('resultats.ply')\n\n    afficher(new_ply_data)","metadata":{"id":"f983d221"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"afficher_exemple(model, afficher_dataloader)","metadata":{"id":"328a0788","outputId":"8c44df5e-42c1-49fa-b74a-d43a3cd57863"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Amélioration\n\nOn va ajouter une couche intermédiaire en plus des batch normalization et des dropouts.","metadata":{"id":"c20c8059"}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, k):\n        super().__init__()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(k*3+3, k),\n            nn.BatchNorm1d(k),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(k, k),\n            nn.BatchNorm1d(k),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(k, 2)\n        )\n\n    def forward(self, x):\n        x = self.linear_relu_stack(x)\n        return x","metadata":{"id":"3b4cbe70"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(k).to(device)\n\nlearning_rate = 0.001\nnum_epochs = 100\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\npatience = 5\n\ntrain_losses = []\nval_losses = []\n\ntrain(model, num_epochs, train_dataloader, afficher_dataloader, criterion, optimizer, train_losses, val_losses, patience)\nafficher_loss(train_losses, val_losses)\nresultat(model, validation_dataloader)","metadata":{"id":"155cc3b9","outputId":"1247b99e-dd10-4824-ca34-3892f6f2f81f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"afficher_exemple(model, afficher_dataloader)","metadata":{"id":"be100b92"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"6d85b055"},"execution_count":null,"outputs":[]}]}